---
title: "Aplicación del Análisis de Cluster a Variables Económicas y Vinculadas al COVID-19"
subtitle: "Análisis Multivariado, Proyecto de Fin de Curso"
author: "Emanuelle Marsella, Maximiliano Saldaña"
date: "Julio, 2020"
output: pdf_document
toc: no
pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]

header-includes:
  - \usepackage{float}
  - \usepackage[spanish]{babel}

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,
                      include=FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      fig.align = 'center',
                      out.extra = '',
                      fig.hold = 'hold',
                      out.width = "50%"
                      )
options(xtable.comment=FALSE,
        xtable.table.placement="H")
```





\newpage


```{r, include = FALSE}
library(dplyr)
library(ggplot2)
library(readxl)
library(xtable)
library(FactoMineR)
library(forcats)
library(gridExtra)
library(factoextra)
library(ggdendro)
library(NbClust)
library(cluster)
library(MASS) 
library(heplots) 
library(StatMatch)
library(GGally)


source("indicadores.R")
source("testes.R")
```




```{r lectura de datos, echo=FALSE}
#Lectura de los datos

datos <- read.table("corona.txt", sep = "\t", header = TRUE)

datos <- datos  %>% rename(healthexp= "currenthealthexpenditureofgdpshx", gdppercap = "gdppercapitaconstant2010us" )
```

```{r imputacion, echo=FALSE}

dim(datos)
(NAcheck <- as.numeric(apply(is.na(datos), 2, sum)))

#Hay NAs en varias variables, una opcion de imputacion, en las cuantitativas imputar la media y en las cualitativas el modo. Stringency tiene 93 NAs (evaluar sacarla)

#vector de valores medios

vals_meds <- rep(0, dim(datos)[2])

for(j in 1:dim(datos)[2]){
  
  vals_meds[j] <- mean(datos[,j], na.rm = TRUE)
}

#imputacion de vals medios en NAs

for(j in 1:dim(datos)[2]){
  if(NAcheck[j]>0){
    
    for(i in 1:dim(datos)[1]){
      if(is.na(datos[i,j])==TRUE){
       datos[i,j] <- vals_meds[j] 
    }
    }
  }
}

```

```{r}

datos <- datos %>% 
  mutate(noinfectionrate = 1-infectionrate)

vars.acp <- c("medage", "gdppercap", "healthexp", "hospitalbed", "noinfectionrate", "CESI_INDEX")



acp <- PCA(datos[, vars.acp], quanti.sup = 6)


```

# Resumen ejecutivo

# Introducción

Se busca aplicar el análisis de cluster al conjunto de países estudiado en (Ceyhun Elgin, Gokce Basbug, Abdullah Yalaman; 2020), con la intención de clasificarlos y hacer una tipología según el conjunto de variables de índole económica y sanitaria que se dispone.

# Análisis de Cluster

Inicialmente son planteados varios tipos de análisis de cluster y se analiza mediante las siluetas que tan buena es la clasificación. Se aplica cluster agregativo, empleando el método de ward y el del vecino más cercano (este último permite observar más facilmente los atípicos), también no jerárquico considerando los centroides, medoides y fuzzy. Se tienen en cuenta conjuntos distintos de variables, las originales de la base, el conjunto empleado para realizar análisis de componentes principales en Marsella y Saldaña (2021) y otro compuesto por las componentes principales obtenidas en ese mismo trabajo.



Dado que la cantidad de observaciones con las que se cuenta originalmente (166) no es muy elevada, una primera opción a la hora de aplicar los métodos de clusters es aplicar clusters jerárquicos. En este método se trabaja con particiones encajadas del conjunto de observaciones, pudiendo ser el método de carácter agregativo o divisivo. En el primer caso se parte de un conjunto de I clusters donde cada uno está formado por una observación y se llega en el paso final a un cluster con las I observaciones. Por otra parte, en el caso de los clusters jerárquicos divisivos se parte de un grupo de I observaciones y se llega a I grupos donde cada uno está conformado por una observación. Esta última estrategia de agrupación puede resultar mejor para casos con gran número de observaciones, en el conjunto de datos no contamos con esta situación por lo que podríamos seguir con la estrategia agregativa. En ambos métodos se debe definir una métrica para determinar qué tan cerca está un objeto (individuos o clusters) de los otros, que para ser calculadas seran tomados en cuenta los valores de las variables de las observaciones. Adicionalmente debe ser elegido un algoritmo para unir grupos entre si y observaciones a grupos.

Como conjunto inicial de variables para agrupar las observaciones elegimos:

* *fiscal* 
* *ratecut*
* *macrofin* 
* *bopgdp* 
* *medage* 
* *gdppercap*
* *healthexp* 
* *hospitalbed*
* *infectionrate*

Que es casi todo el conjunto empleado en Marsella y Saldaña (2021) para el análisis de componentes principales donde se replicó el índice de estímulo económico y el posterior ACP que se hizo con dicho índice y el resto de las variables. No se consideran en este primer intento de conformar clusters las variables *otherbop* y *othermonetary* por ser cualitativas y entonces no poder aplicarse la misma distancia que en el resto de las variables (posteriormente se verá como incluirlas) , *stringency* por su cantidad de valores faltantes ya discutida en el proyecto anteriormente citado y *totalcases* por ser una variable cuya información ya está expresada en *infectionrate* y porque dificulta la comparación entre observaciones al estar medida en términos absolutos y no relativos.


```{r correlaciones}
vars_cluster <- vars.acp
vars_cluster1 <- c("fiscal", "ratecut", "macrofin", "bopgdp", "medage", "gdppercap", "healthexp", "hospitalbed", "infectionrate")

round(cor(datos[,vars_cluster1], ),2)
```

Se debe destacar que las variables *fiscal* y *gdppercap* tienen una correlación de 0.6 mientras que *medage* y *hospitalbed* tienen una correlación de 0.71. Para considerar estas correlaciones más altas que el resto en la formación de clusters se puede emplear como distancia la de Mahalanobis, que incluye la matriz de varianzas y covarianzas en su fórmula que es:

$$d_{ij}^2 = (x_{ik} - x_{jk})'\Sigma^{-1}(x_{ik} - x_{jk})$$
Donde $i$ y $j$ son dos individuos y $\Sigma$ es la matriz de varianzas y covarianzas de las variables. 


Como fue anteriormente mencionado, además de la métrica es necesario optar por un algoritmo para definir como unir grupos y observaciones a grupos. Inicialmente se considerará el método de Ward, el cuál se basa en agregar grupos minimizando la variabilidad dentro de los nuevos grupos formados, que necesariamente aumenta al realizar la unión de objetos heterogeneos pero se busca que aumente lo menos posible. Como contraparte se busca maximizar la varianza entre los grupos en la nueva estructura, así lograr clusters lo más heterogeneos posibles entre si.




En el caso de los clusters jerárquicos necesitamos tomar la decisión de qué estructura de grupos es la elegida, esto es por cuántos grupos se opta. Para hacer esto se recurre a las reglas de detención, que pueden ser globales (evaluán la bondad de particionar en un número determinado de clusters) o locales (sirven para analizar si al unir dos grupos la estructura representa una mejoría).   



Considerando este conjunto de variables, tanto conservando o exluyendo la observación que en el primer proyecto se encontró influyente (San Marino) los indicadores (ver valores) nos llevan a quedarnos con 3 clusters. Pero a la hora de analizar la silueta vemos que en los grupos 2 y 3 hay observaciones que se encuentran en promedio más cercanos en didstancia a las observaciones de alguno de los otros grupos.





+ En este análisis empleamos las componentes principales obtenidas en el primer proyecto como variables para agrupar las observaciones.

```{r clust acp}
datos_acp <- acp$ind$coord[,1:2]

datos_acp <- cbind(as.data.frame(datos_acp), "CESI_INDEX" = datos[,c("CESI_INDEX")])

datos_acp_st <- scale(datos_acp)
cov_datos_acp <- cov(datos_acp_st)

rownames(datos_acp_st) <- datos$country

dist_maha <- mahalanobis.dist(datos_acp_st)


cluster_acp <- agnes(dist_maha, method = "ward", diss = TRUE)
cluster_acp2 <- agnes(dist_maha, method = "single", diss = TRUE)
cluster_acp3 <- agnes(dist_maha, method = "complete", diss = TRUE)

```


```{r dendrograma mas cercano}
fviz_dend(cluster_acp2, cex = 0.3)
fviz_dend(cluster_acp3, cex = 0.3)

#aca se pueden notar los outliers, optamos por usar ward
```


```{r dendrograma ward}
fviz_dend(cluster_acp,cex=0.4)

```

```{r indicadores}
cluster_acp_inds <- indicadores(cluster_acp$merge ,datos_acp ,9 )
```


```{r siluetas}
silueta_acp <- silhouette(cutree(cluster_acp , 2) , dist_maha)
silueta_acp2 <- silhouette(cutree(cluster_acp , 4) , dist_maha)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta_acp, print.summary = TRUE)
```


```{r}
grupos_acp <- cutree(cluster_acp, 2)

graf <- as.data.frame(cbind(datos_acp, grupos_acp))

ggplot(graf) + geom_point(aes(x=Dim.1,y= Dim.2, color = as.factor(grupos_acp)))


```




```{r}
sil <- rep(0,166)
sil[which(silueta_acp[1:166,3]<0)] <- 1

graf <- cbind(datos$country, graf, sil)

filter(graf, sil == 1)
```




# Análisis de discriminante

```{r}
ggpairs(datos_acp,  mapping = ggplot2::aes(color = as.factor(grupos_acp) , alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.9)))
```

```{r tests norm}
testes(datos_acp, as.factor(graf$grupos_acp))
#rechazamos la multinormalidad, la homoscedasticidad y la igualdad de medias
```


```{r logistico acp}

datos_disc <- as.data.frame(cbind(datos[,vars_cluster], graf[,-3]))

log <- glm(as.factor(grupos_acp) ~ gdppercap + noinfectionrate  , family=binomial, data=datos_disc) 
summary(log)

D1 <- log$null.deviance-log$deviance
pchisq(D1, df = 5, lower.tail = F)



respuesta <-predict.glm(log,type='response') #Tenemos la respuesta del modelo


prediccion <- ifelse(respuesta > 0.5,2 ,1)#A los que tengan un score de mas de 0.5 le asigna 1 a los otros 0

#Tabla de confusion
tabla<-table(datos_disc$grupos_acp, prediccion)
tabla
```



```{r}
log2 <- glm(as.factor(grupos_acp) ~  medage + noinfectionrate + healthexp + hospitalbed + CESI_INDEX  , family=binomial, data=datos_disc) 
summary(log2)


log2step <- step(log2)
summary(log2step)



respuesta2 <-predict.glm(log2,type='response') #Tenemos la respuesta del modelo


prediccion2 <- ifelse(respuesta2 > 0.5,2 ,1)#A los que tengan un score de mas de 0.5 le asigna 1 a los otros 0

#Tabla de confusion
tabla2<-table(datos_disc$grupos_acp, prediccion2)
tabla2
```






# Conclusiones

# Anexos

## Anexo 1: Método de cluster alternativos

### Cluster con todas las variables
```{r cluster1}


#ver que hacer con las cualitativas
#ver el conjunto de variables que se eligen para hacer grupos en esta primera en general
#hablar de covarianzas referenciando al primer proyecto, puede afectar si son altas


#primer cluster, la función ya de por si estandariza
clust1 <- agnes(datos[,vars_cluster1], method = "ward", diss = FALSE, stand = TRUE)
clust1.1<- agnes(datos[-130, vars_cluster1], method = "ward", diss = FALSE, stand = TRUE)

```


```{r dendrograma1}
#con san marino
fviz_dend(clust1,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust1_indicadores <- indicadores(clust1$merge ,datos[,vars_cluster1] ,10 )


distancias1 <- get_dist(datos[,vars_cluster1], method = "euclidean", stand = TRUE)
silueta1<-silhouette(cutree(clust1 ,3) , distancias1)

fviz_silhouette(silueta1, print.summary = TRUE)
```


```{r dendrograma1.1}
#con san marino
fviz_dend(clust1.1,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust1.1_indicadores <- indicadores(clust1.1$merge ,datos[-130,vars_cluster1] ,10 )


distancias1.1 <- get_dist(datos[-130,vars_cluster1], method = "euclidean", stand = TRUE)
silueta1.1<-silhouette(cutree(clust1.1 ,3) , distancias1.1)

fviz_silhouette(silueta1.1, print.summary = TRUE)
```


### Cluster CESI_INDEX y otras variables

```{r cluster}

row.names(datos) <- datos$country

datos_clustr <- scale(datos[,vars_cluster])

#primer cluster, la función ya de por si estandariza
cluster <- agnes(datos_clustr, method = "ward", diss = TRUE)

```


```{r dendrograma}
fviz_dend(cluster,cex=0.4)

```



```{r }
fviz_dend(clust2,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust2_indicadores <- indicadores(clust2$merge ,datos[,vars_cluster2] ,10 )


grupos2 <- cutree(clust2, 2)

datos$grupo2 <- grupos2

datos %>% filter(grupos2 == 2)
datos %>% filter(grupos2 == 1)

datos %>% arrange(desc(infectionrate))


distancias <- get_dist(datos[,vars_cluster2], method = "euclidean", stand = TRUE)
silueta2<-silhouette(cutree(clust2 , 2) , distancias)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta2, print.summary = TRUE)


#vemos cuales son los paises con valores negativos de la silueta
a <- rep(0,166)
a[which(silueta2[1:166,3]<0)] <- 1

datos$aux <- a


datosaux <- datos %>% filter(grupos2 == 3)
datosaux %>% filter(aux==1)

#se puede apreciar que considerando la variable infectionrate se diferencian en varianza del conjunto del resto de los paises y los valores se acumulan mas cercanamente al cero. 
ggplot(datosaux)+geom_boxplot(aes(x=as.factor(aux), y = infectionrate))
ggplot(datosaux)+geom_boxplot(aes(x=as.factor(aux), y = gdppercap))

ggplot(datosaux) + geom_point(aes(x = as.factor(aux), y = infectionrate))




asd <- as.data.frame(cbind(acp4$ind$coord[,1:2],  "grupo"  = grupos2[-130], "silueta_neg" = as.factor(datos$aux)))



asd %>% 
  filter(grupo == 2) %>% 
ggplot() + geom_point(aes(x = Dim.1, y = Dim.2, color = as.factor(silueta_neg) ))



```





```{r discriminante_orig}
#grafico inicial
ggpairs(datos[c(vars_cluster2, "grupo2")],  mapping = ggplot2::aes(color = as.factor(grupo2) , alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.9)))

#puede que la normalidad este fallando por la manera que se construyeron los grupos (en los clusters hay paises con silueta negativa, clasificados incorrectamente, lo que puede generar colas pesadas y/o otros modos)


#hacemos los tests de normalidad, homoscedasticidad e igualdad de medias para ver si se puede aplicar discriminante lineal
testes(datos[,c(vars_cluster2)], as.factor(datos$grupo2))


#rechazamos la normalidad, la igualdad de las medias y la homoscedasticidad
```



```{r logistico}
log1<-glm(as.factor(grupo2) ~ medage + gdppercap + healthexp + hospitalbed + infectionrate + CESI_INDEX , family=binomial, data=datos[,c(vars_cluster2, "grupo2")]) 

log1.1 <- stepAIC(log1)


```


```{r}

```
### Cluster vecino más cercano

```{r vecinomcerc}
clust2.1 <- agnes(datos[,vars_cluster2], method = "single", diss = FALSE, stand = TRUE)

clust2.2 <- agnes(datos[,vars_cluster2], method = "complete", diss = FALSE, stand = TRUE)

#se puede notar a san marino como outlier
fviz_dend(clust2.1,cex=0.4)


#se pueden notar que san marino islandia y luxemburgo son los ultimos en agruparse
fviz_dend(clust2.2,cex=0.4)

clust2.3 <- agnes(datos[-c(130, 92,70),vars_cluster2], method = "complete", diss = FALSE, stand = TRUE)

fviz_dend(clust2.3,cex=0.4)

clust2.3_nroclust <- indicadores(clust2.3$merge ,datos[-c(130, 92,70),vars_cluster2] ,10 )



distancias3 <- get_dist(datos[-c(130, 92,70),vars_cluster2], method = "euclidean", stand = TRUE)
silueta2.3<-silhouette(cutree(clust2.3 , 3) , distancias3)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta2.3, print.summary = TRUE)


#con este metodo de union de grupos no obtenemos buenos resultado
```




### Cluster no jerarquico

```{r clustnojer}
set.seed(3652)


datos2$grupo1 <- cutree(clust1, 3)

centroide1 <- datos2 %>% filter(grupo1 == 1) %>% select(vars_cluster1) %>% colMeans()
centroide2 <- datos2 %>% filter(grupo1 == 2) %>% select(vars_cluster1) %>% colMeans()
centroide3 <- datos2 %>% filter(grupo1 == 3) %>% select(vars_cluster1) %>% colMeans()

clusnoj <- kmeans(datos[,vars_cluster1], centers = rbind(centroide1, centroide2, centroide3))
summary(clusnoj)



compnoj <- as.data.frame(cbind(acp4$ind$coord[,1:2],  "grupo"  = as.factor(clusnoj[-130]$cluster))) 



ggplot(compnoj) + geom_point(aes(x = Dim.1, y = Dim.2, color = as.factor(grupo) ))



distancias4 <- daisy(datos[,vars_cluster1], metric = "euclidean", stand = TRUE)
silueta4<-silhouette(clusnoj$cluster , distancias4)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta4, print.summary = TRUE)

```

```{r fuzzy y medoides}

medoid <- pam(datos[,vars_cluster1],2)

distancias5<- get_dist(datos[,vars_cluster2], method = "euclidean", stand = TRUE)
silueta5<-silhouette(medoid$clustering , distancias5)


fviz_silhouette(silueta5, print.summary = TRUE)



fuzz <- fanny(datos[,vars_cluster2], k = 2)
```


### Cluster con componentes principales (sin CESI_INDEX)

```{r clust acp2}
datos_acp2 <- acp$ind$coord[,1:2]

datos_acp2 <- cbind(as.data.frame(datos_acp))

datos_acp_st2 <- scale(datos_acp2)
cov_datos_acp2 <- cov(datos_acp_st2)

rownames(datos_acp_st2) <- datos$country

dist_maha2 <- mahalanobis.dist(datos_acp_st2)


cluster_acp2 <- agnes(dist_maha2, method = "ward", diss = TRUE)
```


```{r dendrograma2}
fviz_dend(cluster_acp2,cex=0.4)

```


```{r indicadores2}
cluster_acp_inds2 <- indicadores(cluster_acp2$merge ,datos_acp2 ,9 )
```



```{r siluetas2}
silueta_acp3<- silhouette(cutree(cluster_acp2 , 2) , dist_maha2)
silueta_acp4 <- silhouette(cutree(cluster_acp2 , 4) , dist_maha2)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta_acp4, print.summary = TRUE)
```

# Referencias