---
title: "Aplicación del Análisis de Cluster a Variables Económicas y Vinculadas al COVID-19"
subtitle: "Análisis Multivariado, Proyecto de Fin de Curso"
author: "Emanuelle Marsella, Maximiliano Saldaña"
date: "Julio, 2020"
output: pdf_document
toc: no
pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]

header-includes:
  - \usepackage{float}
  - \usepackage[spanish]{babel}

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,
                      include=FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      fig.align = 'center',
                      out.extra = '',
                      fig.hold = 'hold',
                      out.width = "50%"
                      )
options(xtable.comment=FALSE,
        xtable.table.placement="H")
```





\newpage


```{r, include = FALSE}
library(dplyr)
library(ggplot2)
library(readxl)
library(xtable)
library(FactoMineR)
library(forcats)
library(gridExtra)
library(factoextra)
library(ggdendro)
library(NbClust)
library(cluster)
library(MASS) 
library(heplots) 
library(MVN) 
library(GGally)


source("indicadores.R")
source("testes.R")
```




```{r lectura de datos, echo=FALSE}
#Lectura de los datos

datos <- read.table("corona.txt", sep = "\t", header = TRUE)

datos <- datos  %>% rename(healthexp= "currenthealthexpenditureofgdpshx", gdppercap = "gdppercapitaconstant2010us" )
```

```{r imputacion, echo=FALSE}

dim(datos)
(NAcheck <- as.numeric(apply(is.na(datos), 2, sum)))

#Hay NAs en varias variables, una opcion de imputacion, en las cuantitativas imputar la media y en las cualitativas el modo. Stringency tiene 93 NAs (evaluar sacarla)

#vector de valores medios

vals_meds <- rep(0, dim(datos)[2])

for(j in 1:dim(datos)[2]){
  
  vals_meds[j] <- mean(datos[,j], na.rm = TRUE)
}

#imputacion de vals medios en NAs

for(j in 1:dim(datos)[2]){
  if(NAcheck[j]>0){
    
    for(i in 1:dim(datos)[1]){
      if(is.na(datos[i,j])==TRUE){
       datos[i,j] <- vals_meds[j] 
    }
    }
  }
}

```

```{r}
vars.acp3 <- c("medage", "gdppercap", "healthexp", "hospitalbed", "infectionrate", "CESI_INDEX")
acp4 <- PCA(datos[, vars.acp3], ind.sup = 130, quanti.sup = 6)


```

# Resumen ejecutivo

# Introducción

Se busca aplicar el análisis de cluster al conjunto de países estudiado en (Ceyhun Elgin, Gokce Basbug, Abdullah Yalaman; 2020), con la intención de clasificarlos y hacer una tipología según el conjunto de variables de índole económica y sanitaria que se dispone.

# Análisis de Cluster

+ Inicialmente son planteados varios tipos de análisis de cluster y se analiza mediante las siluetas que tan buena es la clasificación.

+ Se aplica cluster agregativo, empleando el método de ward y el del vecino más cercano (este último permite observar más facilmente los atípicos), también no jerárquico considerando los centroides, medoides y fuzzy. Se tienen en cuenta conjuntos distintos de variables, las originales de la base, el conjunto empleado para realizar análisis de componentes principales en Marsella y Saldaña (2021) y otro compuesto por las componentes principales obtenidas en ese mismo trabajo.

<!-- (tiene sentido emplear solo las 2 componentes del acp?) -->

## Cluster jerárquico

Dado que la cantidad de observaciones con las que se cuenta originalmente (166) no es muy elevada, una primera opción a la hora de aplicar los métodos de clusters es aplicar clusters jerárquicos. En este método se trabaja con particiones encajadas del conjunto de observaciones, pudiendo ser el método de carácter agregativo o divisivo. En el primer caso se parte de un conjunto de I clusters donde cada uno está formado por una observación y se llega en el paso final a un cluster con las I observaciones. Por otra parte, en el caso de los clusters jerárquicos divisivos se parte de un grupo de I observaciones y se llega a I grupos donde cada uno está conformado por una observación. Esta última estrategia de agrupación puede resultar mejor para casos con gran número de observaciones, en el conjunto de datos no contamos con esta situación por lo que podríamos seguir con la estrategia agregativa. En ambos métodos se debe definir una métrica para determinar qué tan cerca está un objeto (individuos o clusters) de los otros, que para ser calculadas seran tomados en cuenta los valores de las variables de las observaciones. Adicionalmente debe ser elegido un algoritmo para unir grupos entre si y observaciones a grupos.

### Primer análisis: Variables originales

Como conjunto inicial de variables para agrupar las observaciones elegimos:

* *fiscal* 
* *ratecut*
* *macrofin* 
* *bopgdp* 
* *medage* 
* *gdppercap*
* *healthexp* 
* *hospitalbed*
* *infectionrate*

Que es casi todo el conjunto empleado en Marsella y Saldaña (2021) para el análisis de componentes principales donde se replicó el índice de estímulo económico y el posterior ACP que se hizo con dicho índice y el resto de las variables. No se consideran en este primer intento de conformar clusters las variables *otherbop* y *othermonetary* por ser cualitativas y entonces no poder aplicarse la misma distancia que en el resto de las variables (posteriormente se verá como incluirlas) , *stringency* por su cantidad de valores faltantes ya discutida en el proyecto anteriormente citado y *totalcases* por ser una variable cuya información ya está expresada en *infectionrate* y porque dificulta la comparación entre observaciones al estar medida en términos absolutos y no relativos.


```{r correlaciones}
vars_cluster1 <- c("fiscal", "ratecut", "macrofin", "bopgdp", "medage", "gdppercap", "healthexp", "hospitalbed", "infectionrate")
vars_cluster2 <- vars.acp3

round(cor(datos[,vars_cluster1], ),2)
```

Se debe destacar que las variables *fiscal* y *gdppercap* tienen una correlación de 0.6 mientras que *medage* y *hospitalbed* tienen una correlación de 0.71. Para considerar estas correlaciones más altas que el resto en la formación de clusters se puede emplear como distancia la de Mahalanobis, que incluye la matriz de varianzas y covarianzas en su fórmula que es:

$$d_{ij}^2 = (x_{ik} - x_{jk})'\Sigma^{-1}(x_{ik} - x_{jk})$$
Donde $i$ y $j$ son dos individuos y $\Sigma$ es la matriz de varianzas y covarianzas de las variables. 

<!-- como nos afecta tener correlacion alta entre variables? -->

Emplear esta distancia es equivalente a estandarizar los datos y después aplicar la distancia euclidea, cuya fórmula es:

$$d_{ij}^2 = \sum_{k=1}^J(x_{ik} - x_{jk})^2$$
Donde $J$ es el número de variables. Como fue anteriormente mencionado, además de la métrica es necesario optar por un algoritmo para definir como unir grupos y observaciones a grupos. Inicialmente se considerará el método de Ward, el cuál se basa en agregar grupos minimizando la variabilidad dentro de los nuevos grupos formados, que necesariamente aumenta al realizar la unión de objetos heterogeneos pero se busca que aumente lo menos posible. Como contraparte se busca maximizar la varianza entre los grupos en la nueva estructura, así lograr clusters lo más heterogeneos posibles entre si.


```{r cluster1}


#ver que hacer con las cualitativas
#ver el conjunto de variables que se eligen para hacer grupos en esta primera en general
#hablar de covarianzas referenciando al primer proyecto, puede afectar si son altas

row.names(datos) <- datos$country

#primer cluster, la función ya de por si estandariza
clust1 <- agnes(datos[,vars_cluster1], method = "ward", diss = FALSE, stand = TRUE)
clust1.1<- agnes(datos[-130, vars_cluster1], method = "ward", diss = FALSE, stand = TRUE)

```


```{r dendrograma1}
#con san marino
fviz_dend(clust1,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust1_indicadores <- indicadores(clust1$merge ,datos[,vars_cluster1] ,10 )


distancias1 <- get_dist(datos[,vars_cluster1], method = "euclidean", stand = TRUE)
silueta1<-silhouette(cutree(clust1 ,3) , distancias1)

fviz_silhouette(silueta1, print.summary = TRUE)
```


```{r dendrograma1.1}
#con san marino
fviz_dend(clust1.1,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust1.1_indicadores <- indicadores(clust1.1$merge ,datos[-130,vars_cluster1] ,10 )


distancias1.1 <- get_dist(datos[-130,vars_cluster1], method = "euclidean", stand = TRUE)
silueta1.1<-silhouette(cutree(clust1.1 ,3) , distancias1.1)

fviz_silhouette(silueta1.1, print.summary = TRUE)
```

En el caso de los clusters jerárquicos necesitamos tomar la decisión de qué estructura de grupos es la elegida, esto es por cuántos grupos se opta. Para hacer esto se recurre a las reglas de detención, que pueden ser globales (evaluán la bondad de particionar en un número determinado de clusters) o locales (sirven para analizar si al unir dos grupos la estructura representa una mejoría).   



Considerando este conjunto de variables, tanto conservando o exluyendo la observación que en el primer proyecto se encontró influyente (San Marino) los indicadores (ver valores) nos llevan a quedarnos con 3 clusters. Pero a la hora de analizar la silueta vemos que en los grupos 2 y 3 hay observaciones que se encuentran en promedio más cercanos en didstancia a las observaciones de alguno de los otros grupos.


### Segundo análisis: CESI_INDEX y otras variables

```{r cluster2}


#primer cluster, la función ya de por si estandariza
clust2 <- agnes(datos[,vars_cluster2], method = "ward", diss = FALSE, stand = TRUE)




```


```{r dendrograma2}
fviz_dend(clust2,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust2_indicadores <- indicadores(clust2$merge ,datos[,vars_cluster2] ,10 )


grupos2 <- cutree(clust2, 2)

datos$grupo2 <- grupos2

datos %>% filter(grupos2 == 2)
datos %>% filter(grupos2 == 1)

datos %>% arrange(desc(infectionrate))


distancias <- get_dist(datos[,vars_cluster2], method = "euclidean", stand = TRUE)
silueta2<-silhouette(cutree(clust2 , 2) , distancias)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta2, print.summary = TRUE)


#vemos cuales son los paises con valores negativos de la silueta
a <- rep(0,166)
a[which(silueta2[1:166,3]<0)] <- 1

datos$aux <- a


datosaux <- datos %>% filter(grupos2 == 2)


#se puede apreciar que considerando la variable infectionrate se diferencian en varianza del conjunto del resto de los paises y los valores se acumulan mas cercanamente al cero. 
ggplot(datosaux)+geom_boxplot(aes(x=as.factor(aux), y = infectionrate))
ggplot(datosaux)+geom_boxplot(aes(x=as.factor(aux), y = gdppercap))

ggplot(datosaux) + geom_point(aes(x = as.factor(aux), y = infectionrate))




asd <- cbind(acp4$ind$coord[,1:2],  "grupo"  = grupos2[-130]) 




ggplot(as.data.frame(asd)) + geom_point(aes(x = Dim.1, y = Dim.2, color = grupo ))



```

Se aprecia que retomando el gráfico de las proyecciones de

<!-- añadir comparacion con usar todas las variables y usar el cesi_index  -->

```{r vecinomcerc}
clust2.1 <- agnes(datos[,vars_cluster2], method = "single", diss = FALSE, stand = TRUE)

clust2.2 <- agnes(datos[,vars_cluster2], method = "complete", diss = FALSE, stand = TRUE)

#se puede notar a san marino como outlier
fviz_dend(clust2.1,cex=0.4)


#se pueden notar que san marino islandia y luxemburgo son los ultimos en agruparse
fviz_dend(clust2.2,cex=0.4)

clust2.3 <- agnes(datos[-c(130, 92,70),vars_cluster2], method = "complete", diss = FALSE, stand = TRUE)

fviz_dend(clust2.3,cex=0.4)

clust2.3_nroclust <- indicadores(clust2.3$merge ,datos[-c(130, 92,70),vars_cluster2] ,10 )



distancias3 <- get_dist(datos[-c(130, 92,70),vars_cluster2], method = "euclidean", stand = TRUE)
silueta2.3<-silhouette(cutree(clust2.3 , 3) , distancias3)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta2.3, print.summary = TRUE)


#con este metodo de union de grupos no obtenemos buenos resultado
```





### Tercer análisis: Componentes principales

+ En este análisis empleamos las componentes principales obtenidas en el primer proyecto como variables para agrupar las observaciones.

```{r clust compprin}
datos_acp <- acp4$ind$coord[,1:2]



rownames(datos_acp) <- datos$country[-130]
clust3<- agnes(datos_acp, method = "ward", diss = FALSE, stand = TRUE)
```

```{r dendrograma3}
fviz_dend(clust2,cex=0.4)

#se puede ver un problema de atipicos que conforman un cluster separado

clust3_nroclust <- indicadores(clust2$merge ,datos_acp ,10 )

```

+ Tomando en cuenta las reglas de detención se podría considerar 3 clusters.

+ Pero tiene sentido emplear el análisis de cluster para este par de variables?


## Cluster no jerarquico

```{r clustnojer}
set.seed(3652)


datos2$grupo1 <- cutree(clust1, 3)

centroide1 <- datos2 %>% filter(grupo1 == 1) %>% select(vars_cluster1) %>% colMeans()
centroide2 <- datos2 %>% filter(grupo1 == 2) %>% select(vars_cluster1) %>% colMeans()
centroide3 <- datos2 %>% filter(grupo1 == 3) %>% select(vars_cluster1) %>% colMeans()

clusnoj <- kmeans(datos[,vars_cluster1], centers = rbind(centroide1, centroide2, centroide3))
summary(clusnoj)



compnoj <- as.data.frame(cbind(acp4$ind$coord[,1:2],  "grupo"  = as.factor(clusnoj[-130]$cluster))) 



ggplot(compnoj) + geom_point(aes(x = Dim.1, y = Dim.2, color = as.factor(grupo) ))



distancias4 <- daisy(datos[,vars_cluster1], metric = "euclidean", stand = TRUE)
silueta4<-silhouette(clusnoj$cluster , distancias4)


#grafico de silueta, vemos que hay un conjunto de observaciones del grupo 2 con valores negativos
fviz_silhouette(silueta4, print.summary = TRUE)

```

```{r fuzzy y medoides}

medoid <- pam(datos[,vars_cluster1],2)

distancias5<- get_dist(datos[,vars_cluster2], method = "euclidean", stand = TRUE)
silueta5<-silhouette(medoid$clustering , distancias5)


fviz_silhouette(silueta5, print.summary = TRUE)



fuzz <- fanny(datos[,vars_cluster1], k = 2)
```


# Análisis de discriminante



```{r discriminante}
#grafico inicial
ggpairs(datos[c(vars_cluster2, "grupo2")],  mapping = ggplot2::aes(color = as.factor(grupo2) , alpha = 0.5), 
        diag = list(continuous = wrap("densityDiag")), 
        lower=list(continuous = wrap("points", alpha=0.9)))

#puede que la normalidad este fallando por la manera que se construyeron los grupos (en los clusters hay paises con silueta negativa, clasificados incorrectamente, lo que puede generar colas pesadas y/o otros modos)


#hacemos los tests de normalidad, homoscedasticidad e igualdad de medias para ver si se puede aplicar discriminante lineal
testes(datos[,c(vars_cluster2)], as.factor(datos$grupo2))


#rechazamos la normalidad, la igualdad de las medias y la homoscedasticidad
```



```{r logistico}
log1<-glm(as.factor(grupo2) ~ medage + gdppercap + healthexp + hospitalbed + infectionrate + CESI_INDEX , family=binomial, data=datos[,c(vars_cluster2, "grupo2")]) 
```


```{r}

```

# Conclusiones

# Anexos

# Referencias